flux.yarn.queue = "compute"
flux.log-level = "ERROR"

flux.input-type = "HIVE"
flux.output-type = "HDFS"

flux.spark.setup-conf += {"key": "spark.sql.crossJoin.enabled", "value": "true"}
flux.spark.setup-conf += {"key": "hive.exec.dynamic.partition.mode","value": "nonstrict"}
flux.spark.setup-conf += {"key": "spark.sql.files.ignoreCorruptFiles","value": "true"}

flux.spark.setup-var +=  {"key": "date_offset","value": "90"}
flux.spark.setup-var +=  {"key": "table_type","value": "FT"}
flux.spark.setup-var +=  {"key": "table_name","value": "SPARK_FT_MSC_TRANSACTION"}
flux.spark.setup-var +=  {"key": "database_table_name","value": "MON.SPARK_FT_MSC_TRANSACTION"}
flux.spark.setup-var +=  {"key": "table_partition","value": "TRANSACTION_DATE"}
flux.spark.setup-var +=  {"key": "begin_slice_value", "value": "-1"}
flux.spark.setup-var +=  {"key": "end_slice_value", "value": "-1"}

flux.name = "LOAD_EXPORT_CATCH_UP_MSC"

flux.has-date-processing = true

flux.slice-value-type = "DAILY"
flux.slice-begin-value = -1
flux.slice-end-value = -1
flux.slice-step-value = 1
flux.slice-begin-value-offset = 0
flux.slice-end-value-offset = 0
flux.slice-has-state-query=true
flux.slice-state-query="""
SELECT IF(COMPLETUDE=0 AND NB_FILES>150000000  AND T3.SOURCES>=3,'OK','NOK')  FROM
(SELECT COUNT(*) NB_FILES FROM CDR.SPARK_IT_CRA_MSC_HUAWEI WHERE CALLDATE = date_add(current_date, ${hivevar:end_slice_value})) T1,
(SELECT COUNT(*) COMPLETUDE FROM (
    SELECT LAG(INDEX, 1) OVER (PARTITION BY MSC_TYPE ORDER BY INDEX) PREVIOUS,INDEX FROM (
        SELECT
            DISTINCT
            CAST(SUBSTRING(SOURCE,11,9) AS INT) INDEX,
            SUBSTRING(SOURCE,5,11) MSC_TYPE
        FROM CDR.SPARK_IT_CRA_MSC_HUAWEI
        WHERE CALLDATE BETWEEN date_add(current_date, ${hivevar:begin_slice_value}) and date_sub(current_date, ${hivevar:end_slice_value}) and length(source) = 37
    )A
)D WHERE INDEX-PREVIOUS >1)T2,
(SELECT
    COUNT(DISTINCT SUBSTRING(SOURCE,5,7)) SOURCES
FROM CDR.SPARK_IT_CRA_MSC_HUAWEI WHERE CALLDATE BETWEEN date_sub(current_date, ${hivevar:begin_slice_value}) and date_sub(current_date, ${hivevar:end_slice_value}))T3
"""
flux.slice-has-filter-query = false
flux.slice-filter-query = """ 
SELECT date_format(CHECK_DATE, 'yyyy-MM-dd') 
FROM (
	SELECT 
	CHECK_DATE, COUNT(DISTINCT FILE_NAME) DIS_FILE
	FROM (
		SELECT
		A.FILE_NAME, A.CHECK_DATE
		FROM
		(
			SELECT replace(CDR_NAME, 'pla_', '') FILE_NAME, CDR_DATE CHECK_DATE 
				FROM CDR.SPARK_IT_ZTE_CHECK_FILE 
				WHERE CDR_DATE BETWEEN DATE_SUB('###SLICE_VALUE###', 0) AND '###SLICE_VALUE###' AND CDR_TYPE = '${hivevar:cdr_type}'
				GROUP BY CDR_DATE, replace(CDR_NAME, 'pla_', '')
			UNION
			SELECT replace(FILE_NAME, 'pla_', '') FILE_NAME, FILE_DATE CHECK_DATE  
				FROM CDR.SPARK_IT_ZTE_CHECK_FILE_ALL 
				WHERE FILE_DATE BETWEEN DATE_SUB('###SLICE_VALUE###', 0) AND '###SLICE_VALUE###' AND FILE_TYPE = '${hivevar:cdr_type}'
				GROUP BY FILE_DATE, replace(CDR_NAME, 'pla_', '') 
		) A
		LEFT JOIN (
			SELECT ${hivevar:it_partition_column} CHECK_DATE, ORIGINAL_FILE_NAME  
				FROM ${hivevar:it_table_name} B
				WHERE ${hivevar:it_partition_column} BETWEEN DATE_SUB('###SLICE_VALUE###', 0) AND '###SLICE_VALUE###'
				GROUP BY ${hivevar:it_partition_column}, ORIGINAL_FILE_NAME
		) B
		ON A.FILE_NAME = B.ORIGINAL_FILE_NAME AND A.CHECK_DATE = B.CHECK_DATE
		WHERE B.ORIGINAL_FILE_NAME IS NULL
	) TAB
	GROUP BY CHECK_DATE
) T
WHERE DIS_FILE=0
"""
flux.slice-date-format = "yyyy-MM-dd"

flux.has-pre-queries = true
flux.has-exec-queries = true
flux.has-post-queries = false

flux.pre-exec-queries += "/PROD/SCRIPTS/EXPORT/CATCH_UP/prequery_export_catch_up_msc.hql"


flux.inline.exec-queries += "add jar hdfs:///PROD/UDF/hive-udf-1.0.jar"
flux.inline.exec-queries += "DROP  TEMPORARY FUNCTION IF EXISTS GENERATE_SEQUENCE_FROM_INTERVALE"
flux.inline.exec-queries += "CREATE TEMPORARY FUNCTION GENERATE_SEQUENCE_FROM_INTERVALE as 'cm.orange.bigdata.udf.GenerateSequenceFromIntervale'"

flux.exec-queries += "/PROD/SCRIPTS/EXPORT/CATCH_UP/query_export_catch_up_msc.hql"

flux.hdfs.output-format = "csv"
flux.hdfs.output-has-header = "true"
flux.hdfs.output-separator = ";"
flux.hdfs.output-mode = "overwrite"
flux.hdfs.output-path = "/PROD/EXPORT/CATCH_UP/MSC"