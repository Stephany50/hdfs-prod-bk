flux.yarn.queue = "compute"
flux.log-level = "ERROR"

flux.input-type = "HIVE"
flux.output-type = "HDFS"
flux.spark.setup-conf += {"key": "spark.sql.crossJoin.enabled", "value": "true"}

flux.name = "LOAD_EXPORT_TEST"

flux.has-date-processing = true

flux.slice-value-type = "DAILY"
flux.slice-begin-value = -7
flux.slice-end-value = -1
flux.slice-step-value = 1
flux.slice-begin-value-offset = 0
flux.slice-end-value-offset = 0
flux.slice-has-state-query=false
flux.slice-has-filter-query = false
flux.slice-date-format = "yyyy-MM-dd"

flux.has-pre-queries = true
flux.has-exec-queries = true
flux.has-post-queries = false

flux.pre-exec-queries += "/PROD/SCRIPTS/EXPORT/prequery_export_test.hql"

flux.exec-queries += "/PROD/SCRIPTS/EXPORT/query_export_test.hql"


flux.hdfs.output-format = "csv"
flux.hdfs.output-has-header = "true"
flux.hdfs.output-separator = "|"
flux.hdfs.output-mode = "overwrite"
flux.hdfs.output-path = "/tmp/testexport/test1/"
